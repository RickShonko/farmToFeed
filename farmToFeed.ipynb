{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec6a111",
   "metadata": {},
   "source": [
    "Farm to Feed Recommender System\n",
    "Challenge: Predict purchase probability and quantity for 1-week and 2-week horizons\n",
    "Evaluation: 50% AUC (binary classification) + 50% MAE (quantity prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63643116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa721c1a",
   "metadata": {},
   "source": [
    " ============================================================================\n",
    " STEP 1: LOAD DATA WITH MEMORY OPTIMIZATION\n",
    " ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741be950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dtypes(df):\n",
    "    \"\"\"Optimize dataframe memory usage by converting to appropriate dtypes\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load smaller datasets\n",
    "print(\"\\n1. Loading customer data...\")\n",
    "customer_df = pd.read_csv('customer_data.csv')\n",
    "customer_df['customer_created_at'] = pd.to_datetime(customer_df['customer_created_at'])\n",
    "print(f\"   Shape: {customer_df.shape}\")\n",
    "print(f\"   Unique customers: {customer_df['customer_id'].nunique()}\")\n",
    "\n",
    "print(\"\\n2. Loading SKU data...\")\n",
    "sku_df = pd.read_csv('sku_data.csv')\n",
    "print(f\"   Shape: {sku_df.shape}\")\n",
    "print(f\"   Unique SKUs: {sku_df['product_unit_variant_id'].nunique()}\")\n",
    "\n",
    "print(\"\\n3. Loading test data...\")\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "test_df['week_start'] = pd.to_datetime(test_df['week_start'])\n",
    "print(f\"   Shape: {test_df.shape}\")\n",
    "print(f\"   Date range: {test_df['week_start'].min()} to {test_df['week_start'].max()}\")\n",
    "\n",
    "print(\"\\n4. Loading sample submission...\")\n",
    "sample_sub = pd.read_csv('SampleSubmission.csv')\n",
    "print(f\"   Shape: {sample_sub.shape}\")\n",
    "\n",
    "# Load training data with memory optimization\n",
    "print(\"\\n5. Loading training data (this may take a moment)...\")\n",
    "print(\"   Using memory-efficient loading...\")\n",
    "\n",
    "# Define dtypes for training data - exact columns from Train.csv\n",
    "train_dtypes = {\n",
    "    'ID': 'str',\n",
    "    'customer_id': 'int32',\n",
    "    'product_unit_variant_id': 'int32',\n",
    "    'qty_this_week': 'float32',\n",
    "    'num_orders_week': 'int16',\n",
    "    'spend_this_week': 'float32',\n",
    "    'purchased_this_week': 'int8',\n",
    "    'product_id': 'int32',\n",
    "    'grade_name': 'category',\n",
    "    'unit_name': 'category',\n",
    "    'product_grade_variant_id': 'int32',\n",
    "    'selling_price': 'float32',\n",
    "    'customer_category': 'category',\n",
    "    'customer_status': 'category',\n",
    "    'Target_qty_next_1w': 'float32',\n",
    "    'Target_purchase_next_1w': 'int8',\n",
    "    'Target_qty_next_2w': 'float32',\n",
    "    'Target_purchase_next_2w': 'int8'\n",
    "}\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv('Train.csv', dtype=train_dtypes, parse_dates=['week_start', 'customer_created_at'])\n",
    "    print(f\"   ✓ Loaded successfully!\")\n",
    "    print(f\"   Shape: {train_df.shape}\")\n",
    "    print(f\"   Memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   Date range: {train_df['week_start'].min()} to {train_df['week_start'].max()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"   ⚠ Train.csv not found. Please upload the file to continue.\")\n",
    "    print(\"   The notebook is ready to process it once uploaded.\")\n",
    "    train_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52623ec",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "STEP 2: EXPLORATORY DATA ANALYSIS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04912842",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n1. Dataset Overview:\")\n",
    "    print(f\"   Total records: {len(train_df):,}\")\n",
    "    print(f\"   Unique customers: {train_df['customer_id'].nunique():,}\")\n",
    "    print(f\"   Unique products: {train_df['product_unit_variant_id'].nunique():,}\")\n",
    "    print(f\"   Date range: {(train_df['week_start'].max() - train_df['week_start'].min()).days} days\")\n",
    "    \n",
    "    # Target distributions\n",
    "    print(\"\\n2. Target Variable Analysis:\")\n",
    "    print(f\"\\n   1-Week Horizon:\")\n",
    "    print(f\"   - Purchase rate: {train_df['Target_purchase_next_1w'].mean()*100:.2f}%\")\n",
    "    print(f\"   - Avg quantity (when purchased): {train_df[train_df['Target_purchase_next_1w']==1]['Target_qty_next_1w'].mean():.2f}\")\n",
    "    print(f\"   - Max quantity: {train_df['Target_qty_next_1w'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\n   2-Week Horizon:\")\n",
    "    print(f\"   - Purchase rate: {train_df['Target_purchase_next_2w'].mean()*100:.2f}%\")\n",
    "    print(f\"   - Avg quantity (when purchased): {train_df[train_df['Target_purchase_next_2w']==1]['Target_qty_next_2w'].mean():.2f}\")\n",
    "    print(f\"   - Max quantity: {train_df['Target_qty_next_2w'].max():.2f}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Purchase rate over time\n",
    "    purchase_rate = train_df.groupby('week_start')['Target_purchase_next_1w'].mean()\n",
    "    axes[0, 0].plot(purchase_rate.index, purchase_rate.values, marker='o', linewidth=2)\n",
    "    axes[0, 0].set_title('Purchase Rate Over Time (1-Week)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Week')\n",
    "    axes[0, 0].set_ylabel('Purchase Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Quantity distribution\n",
    "    qty_data = train_df[train_df['Target_qty_next_1w'] > 0]['Target_qty_next_1w']\n",
    "    axes[0, 1].hist(qty_data.clip(upper=qty_data.quantile(0.95)), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Quantity Distribution (95th percentile)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Quantity')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Customer category distribution\n",
    "    customer_cat = train_df.groupby('customer_category')['Target_purchase_next_1w'].agg(['mean', 'count'])\n",
    "    customer_cat = customer_cat.sort_values('mean', ascending=False)\n",
    "    axes[1, 0].barh(customer_cat.index, customer_cat['mean'], alpha=0.7)\n",
    "    axes[1, 0].set_title('Purchase Rate by Customer Category', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Purchase Rate')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Current week purchase vs future purchase\n",
    "    current_vs_future = train_df.groupby('purchased_this_week')['Target_purchase_next_1w'].mean()\n",
    "    axes[1, 1].bar(['Not Purchased', 'Purchased'], current_vs_future.values, alpha=0.7, color=['coral', 'green'])\n",
    "    axes[1, 1].set_title('Current Week Purchase → Next Week Purchase', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Next Week Purchase Rate')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9b85d",
   "metadata": {},
   "source": [
    " ============================================================================\n",
    " STEP 3: FEATURE ENGINEERING\n",
    " ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29415ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Create comprehensive features for the model\n",
    "    \"\"\"\n",
    "    print(\"\\n   Creating features...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df['week_of_year'] = df['week_start'].dt.isocalendar().week\n",
    "    df['month'] = df['week_start'].dt.month\n",
    "    df['quarter'] = df['week_start'].dt.quarter\n",
    "    df['day_of_year'] = df['week_start'].dt.dayofyear\n",
    "    \n",
    "    # Customer tenure (days since registration)\n",
    "    df['customer_tenure_days'] = (df['week_start'] - df['customer_created_at']).dt.days\n",
    "    df['customer_tenure_weeks'] = df['customer_tenure_days'] / 7\n",
    "    \n",
    "    # Cyclical encoding for seasonality\n",
    "    df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Price and spending features\n",
    "    df['price_log'] = np.log1p(df['selling_price'])\n",
    "    \n",
    "    # Current week behavior features (available in training data)\n",
    "    if is_train:\n",
    "        df['qty_this_week_log'] = np.log1p(df['qty_this_week'])\n",
    "        df['spend_this_week_log'] = np.log1p(df['spend_this_week'])\n",
    "        df['avg_price_this_week'] = df['spend_this_week'] / (df['qty_this_week'] + 1)\n",
    "        df['orders_per_qty'] = df['num_orders_week'] / (df['qty_this_week'] + 1)\n",
    "    \n",
    "    # Label encoding for categorical variables\n",
    "    le_grade = LabelEncoder()\n",
    "    le_unit = LabelEncoder()\n",
    "    le_cust_cat = LabelEncoder()\n",
    "    le_cust_status = LabelEncoder()\n",
    "    \n",
    "    df['grade_encoded'] = le_grade.fit_transform(df['grade_name'].astype(str))\n",
    "    df['unit_encoded'] = le_unit.fit_transform(df['unit_name'].astype(str))\n",
    "    df['customer_category_encoded'] = le_cust_cat.fit_transform(df['customer_category'].astype(str))\n",
    "    df['customer_status_encoded'] = le_cust_status.fit_transform(df['customer_status'].astype(str))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_aggregated_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Create aggregated features based on historical behavior\n",
    "    This function computes features from past data for each customer-product pair\n",
    "    \"\"\"\n",
    "    print(\"\\n   Creating aggregated historical features...\")\n",
    "    \n",
    "    if not is_train:\n",
    "        # For test data, we'll compute features from the training data\n",
    "        return df\n",
    "    \n",
    "    df = df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
    "    \n",
    "    # Customer-level features (rolling windows)\n",
    "    customer_features = df.groupby('customer_id').agg({\n",
    "        'qty_this_week': ['mean', 'std', 'sum', 'max'],\n",
    "        'spend_this_week': ['mean', 'sum'],\n",
    "        'num_orders_week': ['mean', 'sum'],\n",
    "        'purchased_this_week': ['mean', 'sum']\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_features.columns = ['customer_id'] + [\n",
    "        f'customer_{col[0]}_{col[1]}' for col in customer_features.columns[1:]\n",
    "    ]\n",
    "    \n",
    "    # Product-level features\n",
    "    product_features = df.groupby('product_unit_variant_id').agg({\n",
    "        'qty_this_week': ['mean', 'std', 'sum'],\n",
    "        'purchased_this_week': ['mean', 'sum'],\n",
    "        'spend_this_week': ['mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    product_features.columns = ['product_unit_variant_id'] + [\n",
    "        f'product_{col[0]}_{col[1]}' for col in product_features.columns[1:]\n",
    "    ]\n",
    "    \n",
    "    # Customer-Product interaction features\n",
    "    cust_prod_features = df.groupby(['customer_id', 'product_unit_variant_id']).agg({\n",
    "        'qty_this_week': ['mean', 'std', 'sum', 'count'],\n",
    "        'purchased_this_week': ['mean', 'sum'],\n",
    "        'spend_this_week': ['mean', 'sum']\n",
    "    }).reset_index()\n",
    "    \n",
    "    cust_prod_features.columns = ['customer_id', 'product_unit_variant_id'] + [\n",
    "        f'cust_prod_{col[0]}_{col[1]}' for col in cust_prod_features.columns[2:]\n",
    "    ]\n",
    "    \n",
    "    # Merge features\n",
    "    df = df.merge(customer_features, on='customer_id', how='left')\n",
    "    df = df.merge(product_features, on='product_unit_variant_id', how='left')\n",
    "    df = df.merge(cust_prod_features, on=['customer_id', 'product_unit_variant_id'], how='left')\n",
    "    \n",
    "    # Recency features\n",
    "    last_purchase = df[df['purchased_this_week'] == 1].groupby(\n",
    "        ['customer_id', 'product_unit_variant_id']\n",
    "    )['week_start'].max().reset_index()\n",
    "    last_purchase.columns = ['customer_id', 'product_unit_variant_id', 'last_purchase_date']\n",
    "    \n",
    "    df = df.merge(last_purchase, on=['customer_id', 'product_unit_variant_id'], how='left')\n",
    "    df['weeks_since_last_purchase'] = (df['week_start'] - df['last_purchase_date']).dt.days / 7\n",
    "    df['weeks_since_last_purchase'] = df['weeks_since_last_purchase'].fillna(999)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create features\n",
    "    train_df = create_features(train_df, is_train=True)\n",
    "    train_df = create_aggregated_features(train_df, is_train=True)\n",
    "    \n",
    "    print(f\"\\n   ✓ Feature engineering complete!\")\n",
    "    print(f\"   Total features: {train_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f6108",
   "metadata": {},
   "source": [
    " ============================================================================\n",
    "STEP 4: MODEL TRAINING\n",
    " ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train_data, target_col, is_classification=True):\n",
    "    \"\"\"\n",
    "    Train LightGBM model for either classification or regression\n",
    "    \"\"\"\n",
    "    # Define feature columns (exclude IDs, dates, and targets)\n",
    "    exclude_cols = ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', \n",
    "                    'product_id', 'product_grade_variant_id', 'customer_created_at',\n",
    "                    'grade_name', 'unit_name', 'customer_category', 'customer_status',\n",
    "                    'Target_qty_next_1w', 'Target_purchase_next_1w',\n",
    "                    'Target_qty_next_2w', 'Target_purchase_next_2w',\n",
    "                    'last_purchase_date']\n",
    "    \n",
    "    feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Handle any remaining non-numeric columns\n",
    "    X = train_data[feature_cols].copy()\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "            X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "    \n",
    "    X = X.fillna(0)\n",
    "    y = train_data[target_col]\n",
    "    \n",
    "    # Time-based split (last 20% for validation)\n",
    "    split_date = train_data['week_start'].quantile(0.8)\n",
    "    train_mask = train_data['week_start'] < split_date\n",
    "    \n",
    "    X_train, X_val = X[train_mask], X[~train_mask]\n",
    "    y_train, y_val = y[train_mask], y[~train_mask]\n",
    "    \n",
    "    print(f\"\\n   Training set: {len(X_train):,} samples\")\n",
    "    print(f\"   Validation set: {len(X_val):,} samples\")\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    if is_classification:\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'max_depth': 8,\n",
    "            'min_child_samples': 20\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'max_depth': 8,\n",
    "            'min_child_samples': 20\n",
    "        }\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data_lgb,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data_lgb, val_data_lgb],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    if is_classification:\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        print(f\"   Validation AUC: {score:.4f}\")\n",
    "    else:\n",
    "        score = mean_absolute_error(y_val, val_pred)\n",
    "        print(f\"   Validation MAE: {score:.4f}\")\n",
    "    \n",
    "    return model, feature_cols, score\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Train 4 models: 2 for classification (purchase probability), 2 for regression (quantity)\n",
    "    \n",
    "    print(\"\\n1. Training 1-Week Purchase Probability Model (Classification)...\")\n",
    "    model_1w_class, features_1w_class, score_1w_class = train_models(\n",
    "        train_df, 'Target_purchase_next_1w', is_classification=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n2. Training 1-Week Quantity Model (Regression)...\")\n",
    "    # Only train on samples where purchase occurred\n",
    "    train_qty_1w = train_df[train_df['Target_purchase_next_1w'] == 1].copy()\n",
    "    model_1w_qty, features_1w_qty, score_1w_qty = train_models(\n",
    "        train_qty_1w, 'Target_qty_next_1w', is_classification=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Training 2-Week Purchase Probability Model (Classification)...\")\n",
    "    model_2w_class, features_2w_class, score_2w_class = train_models(\n",
    "        train_df, 'Target_purchase_next_2w', is_classification=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n4. Training 2-Week Quantity Model (Regression)...\")\n",
    "    train_qty_2w = train_df[train_df['Target_purchase_next_2w'] == 1].copy()\n",
    "    model_2w_qty, features_2w_qty, score_2w_qty = train_models(\n",
    "        train_qty_2w, 'Target_qty_next_2w', is_classification=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL TRAINING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nValidation Scores:\")\n",
    "    print(f\"  1-Week Purchase (AUC): {score_1w_class:.4f}\")\n",
    "    print(f\"  1-Week Quantity (MAE): {score_1w_qty:.4f}\")\n",
    "    print(f\"  2-Week Purchase (AUC): {score_2w_class:.4f}\")\n",
    "    print(f\"  2-Week Quantity (MAE): {score_2w_qty:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8eec6",
   "metadata": {},
   "source": [
    " ============================================================================\n",
    "STEP 5: GENERATE PREDICTIONS\n",
    " ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(\"\\nPreparing test data...\")\n",
    "    test_df = create_features(test_df, is_train=False)\n",
    "    \n",
    "    # For aggregated features, we need to compute them from training data\n",
    "    # Merge aggregated features from training\n",
    "    print(\"Merging historical features from training data...\")\n",
    "    \n",
    "    # Get aggregated features from training data\n",
    "    customer_agg = train_df.groupby('customer_id')[[col for col in train_df.columns if 'customer_' in col and col != 'customer_id']].first()\n",
    "    product_agg = train_df.groupby('product_unit_variant_id')[[col for col in train_df.columns if 'product_' in col and col != 'product_unit_variant_id']].first()\n",
    "    cust_prod_agg = train_df.groupby(['customer_id', 'product_unit_variant_id'])[[col for col in train_df.columns if 'cust_prod_' in col]].first()\n",
    "    \n",
    "    test_df = test_df.merge(customer_agg, on='customer_id', how='left')\n",
    "    test_df = test_df.merge(product_agg, on='product_unit_variant_id', how='left')\n",
    "    test_df = test_df.merge(cust_prod_agg, on=['customer_id', 'product_unit_variant_id'], how='left')\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X_test = test_df[features_1w_class].copy()\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].dtype == 'object' or X_test[col].dtype.name == 'category':\n",
    "            X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\nGenerating predictions...\")\n",
    "    \n",
    "    # 1-week predictions\n",
    "    pred_1w_prob = model_1w_class.predict(X_test, num_iteration=model_1w_class.best_iteration)\n",
    "    pred_1w_qty_raw = model_1w_qty.predict(X_test, num_iteration=model_1w_qty.best_iteration)\n",
    "    pred_1w_qty = pred_1w_prob * np.maximum(0, pred_1w_qty_raw)  # Quantity weighted by probability\n",
    "    \n",
    "    # 2-week predictions\n",
    "    pred_2w_prob = model_2w_class.predict(X_test, num_iteration=model_2w_class.best_iteration)\n",
    "    pred_2w_qty_raw = model_2w_qty.predict(X_test, num_iteration=model_2w_qty.best_iteration)\n",
    "    pred_2w_qty = pred_2w_prob * np.maximum(0, pred_2w_qty_raw)  # Quantity weighted by probability\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['ID'],\n",
    "        'Target_purchase_next_1w': pred_1w_prob,\n",
    "        'Target_qty_next_1w': pred_1w_qty,\n",
    "        'Target_purchase_next_2w': pred_2w_prob,\n",
    "        'Target_qty_next_2w': pred_2w_qty\n",
    "    })\n",
    "    \n",
    "    # Ensure probabilities are between 0 and 1\n",
    "    submission['Target_purchase_next_1w'] = submission['Target_purchase_next_1w'].clip(0, 1)\n",
    "    submission['Target_purchase_next_2w'] = submission['Target_purchase_next_2w'].clip(0, 1)\n",
    "    \n",
    "    # Ensure quantities are non-negative\n",
    "    submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(0)\n",
    "    submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(0)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREDICTIONS COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSubmission file saved: submission.csv\")\n",
    "    print(f\"Shape: {submission.shape}\")\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(f\"  1-Week Purchase Probability: mean={submission['Target_purchase_next_1w'].mean():.4f}, \"\n",
    "          f\"std={submission['Target_purchase_next_1w'].std():.4f}\")\n",
    "    print(f\"  1-Week Quantity: mean={submission['Target_qty_next_1w'].mean():.4f}, \"\n",
    "          f\"std={submission['Target_qty_next_1w'].std():.4f}\")\n",
    "    print(f\"  2-Week Purchase Probability: mean={submission['Target_purchase_next_2w'].mean():.4f}, \"\n",
    "          f\"std={submission['Target_purchase_next_2w'].std():.4f}\")\n",
    "    print(f\"  2-Week Quantity: mean={submission['Target_qty_next_2w'].mean():.4f}, \"\n",
    "          f\"std={submission['Target_qty_next_2w'].std():.4f}\")\n",
    "    \n",
    "    print(\"\\n✓ All done! Your submission is ready for upload.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WAITING FOR TRAIN.CSV\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nPlease upload Train.csv to continue with model training and predictions.\")\n",
    "    print(\"Once uploaded, re-run this notebook from the beginning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
